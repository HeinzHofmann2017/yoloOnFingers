\newpage
%Kapitelüberschrift
\section{Tests} 
\label{chapter:tests}
\subsection{erste Ansätze}
Am Anfang ging man von der falschen Vorstellung aus, dass die Detektion der Finger über die Variable $\hat{p}_i$ im Output des Neuronalen Netzwerks läuft. (Tabelle \ref{tbl:beschr_kostenfuntion})
Aus diesem Grund wurden verschiedene Tests aufgebaut.

Ein Test ermittlete aufgrund von $\hat{p}_i$ und einem beliebigen Threshold einen Status für jede Gitterzelle. Die Stati waren: 

\begin{itemize}
\item \grqq{}True-Positive\grqq{}
\item \grqq{}True-Negative\grqq{}
\item \grqq{}False-Positive\grqq{}
\item \grqq{}False-Negative\grqq{}
\end{itemize}

Wer das Kapitel \ref{chapter:design_kostenfunktion} gelesen hat, kann jetzt schon schlussfolgern, dass dies keine brauchbaren Resultate liefern konnte.
Denn wenn die Variable $\hat{p}_i$ mit andauerndem Lernen gegen 1 tendiert und nie nach unten korrigiert wird, übersteigt Sie somit jeglichen Threshold und sagt in jeder Gitterzelle einen rechten Zeigefingerspitz voraus. 
Entsprechend ergaben die \grqq{}True-Positives\grqq{} und die \grqq{}False-Positives\grqq{} zusammen irgendwann = 1, während die \grqq{}True-Negatives\grqq{} und die \grqq{}False-Negatives\grqq{} zusammen = 0 ergaben.
Somit war dieser Test Gegenstandslos und man wusste, dass die Variable $\hat{p}_i$ keine Rolle spielen wird, solange nur Labels mit einem Finger verwendet werden und entsprechend nur eine Klasse existiert.

Ein weiterer Test hatte das Ziel, dass jeweils rund um das jeweilige Label ein Kreis mit einem bestimmten Radius gelegt wird.
Die Vorhersagen wurden nun aufgeteilt in Vorhersagen, welche innerhalb des Kreises lagen, und Vorhersagen ausserhalb des Kreises.
Die Frage war nur noch, wie bestimmt man, welche der 7x7 Boundingboxen als \textbf{die} Vorhersage verwendet wurde, welche mit dem Label verglichen wurde.
Die Antwort ist dank dem Wissen über die Aufgabe, welche das Netz erfüllen muss relativ schnell beantwortet.
Denn wir wissen, dass in der Aufgabe, welche gelöst werden soll immer nur eine rechte Zeigefingerspitze pro Bild vorhanden sein wird.
So musste dafür kein Threshold bestimmt werden, sondern es wurde einfach diejenige Boundingbox gewählt, welche die grösste Confidence lieferte.

Mit diesem Ansatz hatte man nun ein Test, der tatsächlich etwas über das Resultat aussagte. 
So wurden verschieden grosse Kreise um die Labels gezogen um Prozentuale aussagen zu deren Genauigkeit zu kriegen.
Allerdings wurde relativ bald klar, dass es keinen grossen Sinn machen würde für jede Genauigkeit einen Kreis zu ziehen und diese dann einzeln auszuwerten.
Weiter wurde von Guido Schuster \cite{PrivateCommunication} in einem Gespräch folgende Bemerkung gemacht:
"Man sollte das Netzwerk darauf testen, worauf man es auch trainiert hatte."
Dieser Bemerkung folgte schliesslich die Schlussfolgerung, dass mit den Tests auch die IOU der Predictions gegenüber den Labels genauer betrachtet werden sollte.

\subsection{letztendlicher Test}
\label{chapter:letztendlicher_test}
Aus den Erkenntnissen der ersten Ansätze konnte ermittelt werden, das der Optimale Output aus den Tests ein Histogram, bzw. eine Wahrscheinlichkeitsdichteverteilung die richtige Wahl wäre.
So konnte der Code relativ schnell so angepasst werden, dass bei einem Testlauf die Distanz von Label zu Prediction (L2-Norm) für jedes Testbild in ein Element eines Vektors gespeichert wurde.
Aus diesem Vektor konnte dann ein schönes Histogramm erstellt werden, aus welchem mit einem Blick gelesen werden kann, wie sich die Distanzen von Predictions zu den Labels über das gesamte Testset verhalten.

Nach Fertigstellung dieses Tests war es ein leichtes dasselbe für die IOU anstelle der Distanz zu machen. 

Die Wahl der besten Prediction wurde ebenfalls nochmals verbessert.
Im Nachherein ist es ein wenig peinlich, dass in diesem Punkt so viel herumexperimentiert und ausprobiert wurde, da in der Gleichung 1 im Yolo-Paper \cite{yolo} klar ersichtlich ist, dass für die Bestimmung der besten Prediction das Produkt aus $\hat{p}_i$ und $\hat{c}_i$ massgebend ist.
Natürlich machte dies aber auch keinen Unterschied mehr, da $\hat{p}_i$ nahezu = 1 war, war das die Prediction aus $\hat{p}_i * \hat{c}_i$ dieselbe wie wenn nur $\hat{c}_i$ verwendet wurde.

Somit war das Training des Netzwerks auf die Variable $\hat{p}_i$ sowie dessen Verwendung bisher nur irreführend und hatte keinen Nutzen.
Für die Zukunft aber ist es wichtig, dass dieses Yolo auch mehrere Klassen vorhersagen kann.
Dadurch macht es Sinn, diese Variable und die damit verbundenen Berechnungen in der Implementation zu belassen. 

Die Wahrscheinlichkeitsdichtefunktion, welche schliesslich bei diesen Tests durch die L2-Distanz erzeugt wurde, hatte eine etwas spezielle Form (siehe Abbildung \ref{img:dist_dichte_improved}), was Anfangs für Verwirrung sorgte. 
Allerdings konnte ein Gespräch mit Guido Schuster \cite{PrivateCommunication} schnell Klarheit bringen, da es sich \grqq{}offensichtlich\grqq{} um eine Rayleigh-Verteilung handelte. 
Eine solche Verteilung entsteht, wenn zwei Gaussverteilte Variablen über die L2-Norm miteinander verbunden werden.
Da genau dies in diesem Test geschieht, war somit dieser Punkt restlos geklärt.

IOU nicht sehr aussagekräftig...(wahrscheinlich schon im Resultat beschrieben.)

\subsection{Fazit}
Bei einem Neuronalen Netzwerk sollte sich früher Gedanken gemacht werden, wie man das Resultat möglichst Praxistauglich testen kann.
Dies wurde in dieser Arbeit klar falsch gemacht.
Man hatte eine funktionierende Kostenfunktion und wollte diese nach Möglichkeit verbessern.
Allerdings ist das Ziel eines neuronalen Netzwerks nicht eine tiefe Kostenfunktion zu haben, sondern den Task wozu es verwendet wird möglichst gut zu erfüllen.
